---
title: "PCA, FA and beyond ---  the Lecture at Mètodes Estadístics 2021"
author: "Albert Satorra"
date: "10/03/2021"
output: 
  beamer_presentation:  
    fig_height: 3.5
    fig_width: 7.5
    keep_tex: yes
    theme: "Boadilla"
    toc: true
    incremental: false
    slide_level: 2
    
---

<!--
scp /Users/albert/FolderMarcdownRmd/MagistralClassM2021Extended.pdf    satorra@84.89.132.1:~/public_html/dades/
--->  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE)
```


# Introd.

## When analyzing  effects among variables, be aware of ... 
 
1. Type of studies:  experimental vs observational 

Observational:   Recording attributes of units, (hopefully) a **random** sample (not a bias one) from  population

Experimental:  **random**  assigment of treatment to units,  possibly  **random** sample  from a population. [experimental data](/Users/albert/graphstables/experimentaldata.png)

2.  Measurement error on variables (EIV regression) 
[SQP](http://sqp.upf.edu)
[reliab1](/Users/albert/courses/Oslo20152016/mtmmMot1.pdf )
<!---
[reliab2](/Users/albert/courses/Oslo20152016/mtmmMot2.pdf )
[reliab3](/Users/albert/courses/Oslo20152016/mtmmMot3.pdf )
--->  
   
   
3. Control  confounders. In experimental studies, (injection of) randomness produces authomatic control   for confounders. In observational studies,   the regression equation controls for potential confounders. However, an excess of controls  may produce  **endogeneity** and bias of regression effects. 

4. Multiple group, missing data, ordinal variables.  
 
<!---
[SQP](http://sqp.upf.edu)
[reliab1](/Users/albert/courses/Oslo20152016/mtmmMot1.pdf )
[reliab2](/Users/albert/courses/Oslo20152016/mtmmMot2.pdf )
[reliab3](/Users/albert/courses/Oslo20152016/mtmmMot3.pdf )
--->  
   
 
 
 <!---
# La practica de l'ACP i FA amb R

[La pràctica de l'ACP i FA amb R: M2019pca.pdf , pp 39-51](http://84.89.132.1/~satorra/dades/M2019pca.pdf) 

## difference between PCA and EFA

```{r, eval=FALSE}

CPA:  F1=PC1,  F2=PC2
Y1=   *F1     +  *F2 +  E1 
Y2=   *F1     +  *F2 +  E2
Y3=   *F1     +  *F2 +  E3
Y4=   *F1     +  *F2 +  E4
Y5=  *F1      +  *F2 +  E5
E1, E2, ... E5   not very correlated (less correlated as numbe of factors increase)
 
EFA:   
Y1=  *F1   +  *F2 +  E1 
Y2=  *F1   +  *F2 + E2
Y3=  *F1    +  *F2 + E3
Y4=  *F1    +  *F2 + E4
Y5=  *F1    +  *F2 +  E5
E1, E2, ... E5  uncorrelated (independent),  Spearman (1903) 

F1, F2 new (latent) variables

factanal(dades, factors=2)

 

CFA:    
Y1=   *F1  +  0F2 +  E1 
Y2=   *F1     +  0F2 + E2
Y3= 0F1     +  *F2 + E3
Y4=  0F1     +  *F2 + E4
Y5=  0F1     +  *F2 + E5
E1, E2, ... E5   uncorrelated (independent!,
     Joreskog 1978, LISREL, EQS, Mplus, sem de stata    ......      SEM   --- lavaan 



```



## measurement error, regress Y on X

```{r, eval=FALSE}
 lm(model1 , data= dades)
 lm(Y ~X)


     Y =  y + E1           fiability k_Y = var(y)/var(Y)
	   X =  x + E2             fiability k_X = var(x)/var(X)

       lm(Y ~X)

  Joreskog (1978)    LISREL 
```

 
 
## OLS multiple regression 

$$Y_i = \alpha + \beta_1 X_{1i}+ \beta_2 X_{2i}+ \epsilon_i$$
$$Y_i = \hat{Y}_i + e_i$$
where 
$$\hat{Y}_i = a + b_1 X_{1i}+ b_2 X_{2i}$$
is the fitted $Y_i$, and 
$$e_i = Y_i - \hat{Y}_i$$
is the ith residual, and $a$, $b_1$, $b_2$ are the OLS estimators of the intercept and the (partial) regression coefficients.

The following variance decomposition holds: 
$$\mbox{var}\, Y =  \mbox{var}\, \hat{Y} +  \mbox{var}\, e$$
that is,  Total variance = Explained + Residual.  

The multiple R2 is 
$$R^2 =   \frac{Explained}{Total}$$ 
 ---> 

# Statistical Paradoxes (sample selection bias )
 
People very unhappy, will on average, get more happy  

Correspondingly,  people very happy, will on average, get less happy 

```{r , echo=FALSE, eval=TRUE}
 
d<-read.table("http://84.89.132.1/~satorra/dades/data0.txt",header=TRUE)
 x<- d[,1];  y=d[,2]

ind1<-  1*(scale(y) <=  scale(x))
plot(scale(x),scale(y), xlim=c(-3,3),ylim=c(-3,3), frame=FALSE, main='The regression effect' , col=ind1+1)
reg<- lm(scale(y) ~ scale(x) )
 abline(c(0,1),lty=3, col="black", lwd=3)
 abline(reg,lty=1, col="blue", lwd=3)
   abline(h=0, col="grey", lwd=3)
 legend(-3,3, lty =c(3,1,1),col=c("black", "blue","grey"), lwd=c(3,3,3), c("45 degree line", "regression line",'beta=0 line'))
 
 abline(v=-2, col="orange", lwd=4, lty=4)
 abline(v=2, col="orange", lwd=4, lty=4)
 
  abline(v=-1, col="green", lwd=2, lty=4)
 abline(v=1, col="green", lwd=2, lty=4)
  
 
#table(ind1[abs(scale(x))<= 1])

#table(ind1[ scale(x)<= -2])

#table(ind1[ scale(x) >= +2])
``````


## The pill for happiness (cont.)
 
 Suppose x is your happiness this week, y  your happiness next week.  Even doing nothing, there is a significant positive correlation between x and y, a correlation that is not one. In this setting, there is room for a "curandero" (healer) effect. Points in red, the points below the 45-degree line, are the people that worsen happiness from this to the next week.   Take the people with the poorest values of happiness this week, the people that are below the first orange vertical line.   To those people that are very unhappy, you give them a pill that is just a placebo.  In the correlation setting of this graph,  where correlation is  `r round(cor(x,y),2)`,  we observe that `r table(ind1[ scale(x)<= -2])[1]` persons improved their happiness next week (they show up above the 45 degree line) while only `r table(ind1[ scale(x)<= -2])[2]` got worst (they show up below the 45 degree line). So  doing nothing, has improved  the life of `r  round(100*prop.table(table(ind1[ scale(x)<= -2])),2)[1]` % of the very unhappy people. 

## ... a pill for happiness (cont.)
This clear improvement in happyness does not happen for the middle range of people,  the points between the two green lines. For those people  
`r table(ind1[ abs(scale(x)) <= 1])[1]` improved while `r table(ind1[ abs(scale(x)) <= 1])[2]` got worst. 

Note that the people that this week feels very happy, points on the right of the right orange line,  the same placebo would "make" them less happy next week, `r  round(100*prop.table(table(ind1[ scale(x)>= 2])),2)[2]`%  would decrease their happiness next week. The same regression effect applies for the people with too high values of x, but now, in the opposite direction.

## Berkson's paradox (another sample selection bias )

Does beauthy and intelligence correlate?  According to the stars in TV pograms that is not the case, in fact correlation is found to be negative!

This is the same effect, as when it was reported that smoking was negatively correlated with Covid (the sample was collected in respiratory units of hospital). 

Bellow, this is violating the back door criterium, by controlling on a colider. 



## Berkson's paradox  (cont.)

```{r, echo=FALSE}
  C<- rnorm(1000)
  x<-rnorm(1000) + C; y<-rnorm(1000)+C
  y=.2*x + rnorm(1000)
  x<- scale(x); y<-scale(y)
 ind2<-  y>=1
 ind1<-  x>=1
 ind<- ind1 | ind2

 #plot(x[ind],y[ind])
 plot(x[ind] ,y[ind], xlab="intelligence", ylab=" beauty", main = "Association between beauty (people) and intelligence" , col="orange", pch="+",cex=0.8)
 points(x[!ind] ,y[!ind],col="blue", cex=0.8)
 abline(lm(y[ind]~ x[ind]), col="red", lty= 3, lwd=3)
 abline(lm(y ~ x ), col="blue", lty= 1,lwd=3)
 legend(1.0, 3.5, lwd=3, lty=c(1,3), col=c("blue","red"), c("in population", "in TV people"))

  
```

 
## using diagram  (3 nodes) 
 [library(diagram)](https://cran.r-project.org/web/packages/diagram/vignettes/diagram.pdf)
```{r, echo=FALSE}
library(diagram)
data <- c(0, "'.47*'", 0,
          0, 0, 0, 
          "'.36*'", "'.33* (.16)'", 0)
M<- matrix (nrow=3, ncol=3, byrow = TRUE, data=data)
plot<- plotmat (M, pos=c(1,2), 
                name= c( "Math SE","Math A", "Interest MM"), 
                box.type = "rect", box.size = 0.08, box.prop=0.5,  curve=0)
## math self-efficacy
## math ability
## interest in the math major

```




##  The back-door criterion

```{r, echo=FALSE, fig.height = 5, fig.width = 6, fig.align = "center"}
 names <- c( "B", "A","Y", "V","W","M" )
   
 M <- matrix(nrow = 6, ncol = 6, byrow =  TRUE, data = 
 0)
 
 Col <- M
 Col[] <- "black"
 Col[3, 2] <- "darkred"
 
   M[1,2]<- "i1";    M[3,2]<- "effect";  M[3,1]<- "i2"
   M[2,4]<- "p1";    M[3,5]<- "p2"; 
   M[6,4]<- "q1";    M[6,5]<- "q2"; 
    #  a<-sample(1:6,6)
      #M<- M[a,a]
 plotmat(M, pos = c(1,2, 2,1), curve = 0, name = names, lwd = 1,
  box.lwd = 2, cex.txt = 0.8, box.type = c("circle", "square","square","circle", "circle", "circle"), 
box.lcol= c(1,1,3,1,1,1),
arr.lcol = Col, box.prop = 0.2)  
```

## Back-door paths

To assess the effect of the intervention do (A = a) on the variable Y, each of the following sets of control variables {B}, {B,V,M}, {B,W,M} are sufficient to identify the 
distribution $p(Y\mid A)$. 

Each of the above sets, satisfy the  *back door criterion* (Judea Pearl (1993))  for the ordered (A,Y) variables. Each of the setsidentify the distribution of $p(Y\mid A)$. 


 
## Back-door criterion: all the back-door paths should be closed  

For proper causal assessment of X into Y,  espurious correlation (confounding) need to be discarded. All the back-doors paths from X to Y  should be closed.  Given a DAG (directed acyclic graph) and the directed path X to Y, a back-door path is a path that contains an arrow into X.  

Closing the back-door  paths of X to Y means setting fixed (under control) a sufficient set of variables Z. We say that Z identifies the effect of X on Y (Z meets the back-door criterion)  when 

1) No descendant of X is on Z
2)  Z blocks every "back door" path between X and Y  

## The Partial Dependency Plot (Friedman 2001)

(see Causal interpretatons of black-Box model Zhao and T. Hastie, 2018)

When back-door criterion is met,  the PDP (partial dependence plot) of Friedman (2001) corresponds to the back door adjustment formula of  Pearl (1993). That is, under the back door criterion, the causal effect of Xs on Y is identifiable and is given by 
$$ P(Y\mid do(Xs = xs))= \int_z P(Y\mid  Xs = xs, Z = z))dP(z)$$ 
The PDF for a function $g_X(xs)$ is

$$ g_X(xs) = E_Z ( g_X(xs,Z) ) = \int_z g_X(xs,Z) ) dP(z)$$

## Controlling for confounders 
 
Closing the back doors depends on the model, the full path diagram model.  Without control for confounders, the correlation can be spurious. Notorious cases are

1.  Simpson's paradox on discrimination by gender -- discrimination be gender in the campus of Berkeley. 

2. The case of coliders, the negative correlation of smoking and   Covid, when the sample of subjects is collected in hospital. The negative correlation of Beauty and inteligence (sample taken from TV shows). 

3.  The case of difference on difference effects of a treatment and health. Wine on living longer and wealth. 
 
## Correlation does not equal causation.    

1.  [Backdoor path criterion (video)](https://www.coursera.org/lecture/crash-course-in-causality/backdoor-path-criterion-Af3e6)



2. [Identification a causal relationship (video)](https://www.youtube.com/watch?v=IajzIKW518M)
  
3. 

[DAGitty](https://www.youtube.com/watch?v=tZe3ZNYCrDQ)

[about DAGitty](https://www.youtube.com/watch?v=prlZ9k5l1s4)

[how we can build our own diagrams](https://www.youtube.com/watch?v=3PdIR4P107k)

## Example (gapminder):   life expectancy and gdpPercap 

```{r}
library(tidyverse)
library(gapminder)
data(gapminder)
cor(gapminder$lifeExp , log(gapminder$gdpPercap) )

gap  <- gapminder %>% group_by(country) %>% 
   mutate(lifeExp.r = lifeExp -mean(lifeExp),
          logGDP.r = log(gdpPercap) - mean(log(gdpPercap))) %>% 
       ungroup()

cor(gap$lifeExp.r,  gap$logGDP.r)
```


## using diagram  ( 6 nodes ) 
 
```{r,echo=FALSE, fig.height = 4, fig.width = 6, fig.align = "center"}


  names <- c("PHYTO", "NH3", "ZOO", "DETRITUS", "BotDET", "FISH")
  M <- matrix(nrow = 6, ncol = 6, byrow = TRUE, data = c(
  #   p n z d b f
  0,1,0, 0, 0, 0,#p
  0,0,4,10,11, 0,#n
  2,0,0, 0, 0, 0,#z
  8,0,13,0, 0,12,#d
  9,0,0, 7, 0, 0,#b
  0,0,5, 0, 0, 0 #f
   ))

pp <- plotmat(M, pos = c(1, 2, 1, 2), curve = 0, name = names,
    lwd = 1, box.lwd = 2, cex.txt = 0.8,
     box.type = c("square", "diamond","circle"), box.prop = 0.5, arr.type = "triangle",
     arr.pos = 0.4, shadow.size = 0.01, prefix = "f",
     main = "NPZZDD model")



phyto <-   pp$comp[names=="PHYTO"]
zoo <- pp$comp[names=="ZOO"]
nh3 <- pp$comp[names=="NH3"]
  detritus <- pp$comp[names=="DETRITUS"]
  fish <- pp$comp[names=="FISH"]
 
```

## using ggraph and igraph (3 nodes)

```{r, echo=FALSE, fig.height = 4, fig.width = 6, fig.align = "center"}
library(ggraph)
require(igraph)

test <- data.frame(from = c(1,2,3), to=c(2,3,1), coef = letters[1:3])
simple <- graph_from_data_frame(test)
V(simple)$name <- LETTERS[1:3]

ggraph(simple, layout = 'auto') + 
  geom_edge_link(aes(label = coef), 
                 angle_calc = 'along',
                 label_dodge = unit(2.5, 'mm'),
                 arrow = arrow(length = unit(4, 'mm')), 
                 end_cap = circle(3, 'mm')) + 
  geom_node_label(aes(label = name),size = 6) +
  theme_graph()
```


## Statistics, be aware of ...  
 
1. "Uncertainty is unconfortable, but not accounting for it is stupid" (old chinese aforism). Risk zero does not exit, but not evaluating it is stupid.  
[First class in probability: ](/Users/albert/graphstables/aforismochinoprobabilidad.png)

That is why the p-values,   confidence bounds statements, ...
 
2. type of data:  experimental vs observational 

Observational:   Recording attributes of units, (hopefully) a **random** sample (not a bias one) from  population

Experimental: we assign **randomly**  treatment (values of a variable) on  possibly a **random** sample  of units.   

[experimental data](/Users/albert/graphstables/experimentaldata.png)

##  Be aware of ...  (cont.)

3. In assessing causality, take care of confounders. In experimental studies, an injection of randomness   controls for confounders. In observational studies, bring in the model all potential confounders (though then,  risk of endogeneity).  
 
4. Account for missing data,  it can severely bias the sample

5. Account for measurement error 


<!---
[SQP](http://sqp.upf.edu)
[reliab1](/Users/albert/courses/Oslo20152016/mtmmMot1.pdf )
[reliab2](/Users/albert/courses/Oslo20152016/mtmmMot2.pdf )
[reliab3](/Users/albert/courses/Oslo20152016/mtmmMot3.pdf )
--->  


 

## Statistics, be aware of ...  (cont.)

6.  When endogeneity, simultaneous equations 

7.  When big data (large n, many variables, authomatic collection of data)  common sense statistics plus computer power. Machines + data + No-human-brain,  nothing of substance!  

8. Some examples:  factor and  
 path models  

<!---
models de fluxes,   atzar sota control, per fer front a atzar descontrolat  (els tallafocs, ajuden a controlar el foc desbocats)
 ---> 
 
# La practica de l'ACP i FA amb R

[La pràctica de l'ACP i FA amb R: M2019pca.pdf , pp 39-51](http://84.89.132.1/~satorra/dades/M2019pca.pdf) 
 
## difference between PCA and EFA

```{r, eval=FALSE}
CPA:  F1=PC1,  F2=PC2
Y1=   *F1     +  *F2 +  E1 
Y2=   *F1     +  *F2 +  E2
Y3=   *F1     +  *F2 +  E3
Y4=   *F1     +  *F2 +  E4
Y5=  *F1      +  *F2 +  E5
E1, E2, ... E5   not very correlated (less correlated as numbe of factors increase)
 
EFA:   
Y1=  *F1   +  *F2 +  E1 
Y2=  *F1   +  *F2 + E2
Y3=  *F1    +  *F2 + E3
Y4=  *F1    +  *F2 + E4
Y5=  *F1    +  *F2 +  E5
E1, E2, ... E5  uncorrelated (independent),  Spearman (1903) 
```

## ... (cont.)

```{r, eval=FALSE}


F1, F2 new (latent) variables

factanal(dades, factors=2)

 

CFA:    
Y1=   *F1  +  0F2 +  E1 
Y2=   *F1     +  0F2 + E2
Y3= 0F1     +  *F2 + E3
Y4=  0F1     +  *F2 + E4
Y5=  0F1     +  *F2 + E5
E1, E2, ... E5   uncorrelated (independent!,
     Joreskog 1978, LISREL, EQS, Mplus, sem de stata    ......      SEM   --- lavaan 



```


## measurement error, regress Y on X

```{r, eval=FALSE}
 lm(model1 , data= dades)
 lm(Y ~X)


     Y =  y + E1           fiability k_Y = var(y)/var(Y)
	   X =  x + E2             fiability k_X = var(x)/var(X)

       lm(Y ~X)

  Joreskog (1978)    LISREL 
```

# Be aware of measurement error  

## OLS multiple regression 

$$Y_i = \alpha + \beta_1 X_{1i}+ \beta_2 X_{2i}+ \epsilon_i$$
$$Y_i = \hat{Y}_i + e_i$$
where 
$$\hat{Y}_i = a + b_1 X_{1i}+ b_2 X_{2i}$$
is the fitted $Y_i$, and 
$$e_i = Y_i - \hat{Y}_i$$
is the ith residual, and $a$, $b_1$, $b_2$ are the OLS estimators of the intercept and the (partial) regression coefficients.

The following variance decomposition holds: 
$$\mbox{var}\, Y =  \mbox{var}\, \hat{Y} +  \mbox{var}\, e$$
that is,  Total variance = Explained + Residual.  

The multiple R2 is 
$$R^2 =   \frac{Explained}{Total}$$ 

## Measurement  error on a Xs 
  

Let $$Y = \alpha + \beta x + \epsilon$$
with the true value x  not observed; instead,
we observe $$X = x + u$$
In that set-up, the OLS regression coefficient estimate is biased toward zero (so-called "attenuated effect" of measurement error). The  $b$ converges in probability not to $\beta$, but to   $\beta \times k_x$, where 
$$k_X =  \frac{var(x)}{var(X)}$$
The value of $k_X$ is known as the _reliability coefficient_ of  X, for measuring the true x.   For simple linear regression the effect is an underestimate of the coefficient, known as the *attenuation bias*. For non-linear models the direction of the bias is likely to be more complicated

## Fuller's  reliability table

Table 1.1.1. of Fuller (1987, p. 8) shows estimates of reliability coefficients for a number of socioeconomic variables.   The estimates of the table were constructed from repeated interview studies conducted by the United States Bureau of the Census. In such studies, the same data are collected in two different interviews, the comparison of responses in the 1970 Census with the same data collected in the Current Population Survey.  In survey sampling, the measurement error in data collected from human respondents is uaually called _response error_  
 

|Variable | k |
|:---| :---: |  ---:
| Sex | .98 | 
| Age | .99 |
| Age (45-49)(0-1) | .92 |
| Education | .88 |  
| Income | .85 | 
| Unemployed | .77 |
| Poverty status | .58 |

  


##  Meas error an attenuation effect

```{r, echo=FALSE, fig.height = 5, fig.width = 6, fig.align = "center"} 
n = 1000
set.seed(2018)
X <- rnorm(n)
Y <- sqrt(0.5) * X + sqrt(0.5) * rnorm(n)
#cor(X, Y)
# Regression fit of y (or Y) on x (or X)
plot(X, Y, xlim = c(-4, 4), ylim = c(-4, 4), main = "", xlab="X is  with error,   x is true value", ylab="Y is with error,  y is true value")
abline(lm(Y ~ X), col = "blue", lwd = 4, lty=1)
#abline(c(0, 1), col = "white", lwd = 4)
legend(-4, 3, c("OLS y on x",  "OLS y on X", "OLS Y on x", "OLS Y on X"), lty=c(1, 2,1,2),lwd = 4, col = c("blue",  "red", "orange", "white"))
r <- as.character(round(cor(Y, X), 2))
text(3, -2, paste("cor(x,y) =", r, sep = " "), col="blue")
XX<-  X + 3*rnorm(n)
r <- as.character(round(cor(Y, XX), 2))
text(3, -2.5, paste("cor(X,y) =", r, sep = " "), col="red")
abline(lm(Y ~ XX), col = "red", lwd = 4, lty=2)
YY<-  Y + 3*rnorm(n)
abline(lm(YY ~ X), col = "orange", lwd = 4, lty=1)
r <- as.character(round(cor(YY, X), 2))
text(3, -3, paste("cor(X,y) =", r, sep = " "), col="orange")
#text(-0.2, -3, paste("k_Y and k_X = ", round(1/(1+ 1.3**2),2), sep = " "), col="gray")
abline(lm(YY ~ XX), col = "white", lwd = 4, lty=2)
r <- as.character(round(cor(YY, XX), 2))
text(3, -3.5, paste("cor(X,Y) =", r, sep = " "), col="white")



```

 


#  Ex.: DRT and Alcohol Intake (AI) 


##   DRT and Alcohol Intake  
 

Consider  the relation of Alcohol Intake (AI) of a driver and the time it takes for the driver to perceive that a signal has occurred and to decide upon a response. For more on Driver Reaction Time (DRT) see [Driver Reaction Time](https://www.visualexpert.com/Resources/reactiontime.html). 
  

##  EiVMalcohol.dat 

```{r}

 
ndata = read.table("http://84.89.132.1/~satorra/dades/EiVMalcohol.dat", header = T)
dim(ndata)
head(ndata)
names(ndata)
attach(ndata)
round(cov(ndata),3)

`````


## Covariance graph

```{r, echo=FALSE,fig.height = 4, fig.width = 6, fig.align = "center"}

  model0<-  "   Resp ~~ Resp + Ointake1 + Ointake2;
           Ointake1 ~~  Ointake1 + Ointake2;
           Ointake2 ~~    Ointake2; " 
 library(lavaan)
  fit0 <- lavaan::sem(model0, data=ndata )
  library(semPlot)
semPaths(fit0, "est", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6, groups = "manifest")

 
```


 
## covariance graph 

```{r, eval=FALSE}
 library(semPlot)
semPaths(fit0, "est", sizeInt = 1, sizeMan = 8, 
    edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, 
    edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   
    nCharNodes = 6, groups = "manifest")
##  style= "lisrel"

```

##  Multiple regression (Ointake data)

```{r, echo=FALSE,fig.height = 4, fig.width = 6, fig.align = "center"}

library(lavaan)
library(semPlot)
####################################### 
data = as.data.frame(cbind(Ointake1, Ointake2, Resp))
pdmodel2 <- " Resp ~  Ointake1 +  Ointake2 "
mlfit2 <- lavaan(pdmodel2, data = data, auto.var = TRUE)
 
# summary(mlfit2)
# parameterEstimates(mlfit2 )
 
semPaths(mlfit2, "est", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6, style="lisrel", groups="manifest")

```

##  Multiple regression (Ointake data): beta coefficients

```{r, echo=FALSE,fig.height = 4, fig.width = 6, fig.align = "center"}


semPaths(mlfit2, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6,style="lisrel",groups="manifest")

```


## Factor Analysis (Ointake data):  factanal()  
 
```{r, echo=TRUE,fig.height = 4, fig.width = 6, fig.align = "center"}
(EFA<-  factanal(data, factors = 1))
  

```

## Path diagram:  one-factor model

```{r, echo=FALSE, fig.height = 3.5, fig.width = 6, fig.align = "center"}
semPaths(EFA, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6)
```

## Reliability of Ointake 1 and Ointake 2

Reliability of  Ointake 1 and Ointake 2 is the squares of the standardized loadings,   $.45**2=0.20$   and $.89**2 =0.8$ respectively. The R2  of  Resp regress on true intake is $.88**2 = .774$.  
Note:  accounting for measurement error we  increased the magnitude of the regression coefficient of intake on the response.  

##  Factor Analysis (Ointake data):  lavaan()  
```{r, fig.height = 3.5, fig.width = 6, fig.align = "center"}
  
library(lavaan)
library(semPlot)
####################################### 
pdmodel2 <- " 
        # latent variable definitions
       Tintake =~  1*Ointake1 + 1*Ointake2
        # regression 
        Resp ~ Tintake
            "
mlfit2 <-  lavaan(pdmodel2, data = data, auto.var = TRUE)


```
 
 
 
## Factor Analysis (Ointake data):  lavaan, semPlot   
 
```{r, echo=FALSE,  fig.height = 3.5, fig.width = 6, fig.align = "center"}
#summary(mlfit2)
#semPaths(mlfit2, "est",   weighted = FALSE, style="lisrel")  
semPaths(mlfit2, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6, style="lisrel", groups="latents")

```
 
The std solution equals the std of factanal(). 


# Ex. Employee data (NAs + path models)

## Regression (employee data) 

```{r, echo=FALSE,fig.height = 3.5, fig.width = 6, fig.align = "center"}
   ### lavaan with missing data
library(lavaan)
data = read.table("http://84.89.132.1/~satorra/dades/employee.dat")
names(data) <- c("id", "age", "tenure", "female", "wbeing", "jobsat", "jobperf", "turnover", "iq")
data[data == -99] <- NA

head(data)

```


## Accounting for NAs,   library(VIM)

```{r, echo=TRUE,fig.height = 3.5, fig.width = 6, fig.align = "center"}
library(VIM);
aggr(data, numbers = TRUE, 
     prop = c(TRUE, FALSE), cex.axis = 0.5)
```


## Moments unrestricted  (M0)

```{r, echo=FALSE,fig.height = 5, fig.width = 6, fig.align = "center"}
## Model 0, moments unrestriced
# Create descriptive model object
model0 <- "
# means
age ~1
tenure ~1
female ~1
wbeing ~1
jobsat ~1
jobperf ~ 1
turnover ~ 1
iq ~1
# variances
age ~~age
tenure ~~ tenure
female~~female
wbeing~~wbeing
jobsat~~jobsat
jobperf  ~~ jobperf
turnover ~~ turnover
iq ~~ iq
       # covariances/correlations
age~~ tenure + female + wbeing + jobsat + jobperf + turnover + iq
tenure~~ female + wbeing + jobsat + jobperf + turnover + iq
wbeing~~  jobsat + jobperf + turnover + iq
jobsat ~~   jobperf + turnover + iq
jobperf ~~  turnover + iq
turnover ~~ iq
"


library(lavaan)

fit0 <- sem(model0, data, missing = "listwise")
# summary(fit2)
```

 
##  Moments unrestricted   (listwise)

```{r, echo=FALSE,fig.height = 3.5, fig.width = 6, fig.align = "center"}

library(semPlot)
semPaths(fit0, "est", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6, groups="manifest")
 

```

 
##  Moments unrestricted   (missing = "fiml") (MAR)

```{r, echo=FALSE,fig.height = 3.5, fig.width = 6, fig.align = "center"}
fit0 <- sem(model0, data, missing = "fiml")

library(semPlot)
semPaths(fit0, "est", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, layout = "circle",
    weighted = FALSE, intercepts = FALSE, edge.label.position = 0.7, bg = "white",
    borders = TRUE, label.norm = "O",   nCharNodes = 6,  groups="manifest")
 

```

# Confounders 

## Be aware of confounders (I) 


```{r, echo=TRUE,fig.height = 3.5, fig.width = 6, fig.align = "center"}
  modela<- "   jobsat  ~   jobperf "   
 (fita <- sem(modela, data, missing = "fiml") )

```

## Be aware of confounders (I)

```{r, echo=FALSE, fig.height = 3.5, fig.width = 6, fig.align = "center"}

semPaths(fita, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, weighted=FALSE,intercepts = FALSE, 
    edge.label.position = 0.6, bg = "white", borders = TRUE,
 label.norm = "O", style = "lisrel", nCharNodes = 6,curve=2,curvature=1.4,combineGroups=FALSE,groups="manifest")
```

## Be aware of confounders (II)

```{r, echo=TRUE,fig.height = 3.5, fig.width = 6, fig.align = "center"}
  modelb<- "   jobperf  ~    jobsat  "    
 
 (fitb <- sem(modelb, data, missing = "fiml") )

```


## Be aware of confounders (II)

```{r, echo=FALSE, fig.height = 3.5, fig.width = 6, fig.align = "center"}

semPaths(fitb, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, weighted=FALSE,intercepts = FALSE, 
    edge.label.position = 0.6, bg = "white", borders = TRUE,
 label.norm = "O", style = "lisrel", nCharNodes = 6,curve=2,curvature=1.4,combineGroups=FALSE,groups="manifest")
```

## Experimental (quasi-experimental) data, IV

![Regression for causal effect](/Users/albert/graphstables/causaleffectSEM.png){width=80% }}

## causal analysis, observatinal vs experimental studies
 
```{r, eval=FALSE}

#Experimental Studies 

V1 is treatment (0/1, coupon or not  training program)
 
v2 is hours in training program  

v3  salary one year later training program 

# multiple regression 

    V3 on v1 v2
# causal model
    
v3 on v2

v2 on v1 

## observational studies 

V3 on v1 v2 and more variables 

V2 on  v1  more variables 

   
```


## Controlling Confounding Bias (1)

[controlling confounding ](http://bayes.cs.ucla.edu/BOOK-2K/ch3-3.pdf)

Whenever we undertake to evaluate the effect of variable A on another Y, the question arises as to whether we should adjust for possible confounding V, W, etc. otherwise known as "covariates", "concomitants" or simply "confounders" (Cox, 1958, p.48).   Adjustments amount to partitiioning the population into groups that are homogeneous relative to V, W, etc. and then assessing the effect of A on Y on each of these groups, and then averaging the results across the groups. This was recognized by Karl Pearson as early as 1899 of what now is called Simpson's paradox. Any true relation between A and Y can be reversed by including  additional variables in the analysis.  For example, we may find that students who smoke obtain higher grades, a relation that is reversed when we control for age, further adjusting for family income, again smokers  obtain higher grades than non-smokers, and so on.   
See discrimination on salary in the Berkeley


## Controlling Confounding Bias (1)
Despite a  more than a century of analysis, Simposon's reversal continues to trap the unwary.  

##  In the early 1970s, Berkeley was sued for gender discrimination over admission to graduate

[Berkeley](https://www.brookings.edu/blog/social-mobility-memos/2015/07/29/when-average-isnt-good-enough-simpsons-paradox-in-education-and-earnings/)

In the early 1970s, the University of California, Berkeley was sued for gender discrimination over admission to graduate school. Of the 8,442 male applicants for the fall of 1973, 44 percent were admitted, but only 35 percent of the 4,351 female applicants were accepted. At first blush, and assuming the applicants’ qualifications were similar, this pattern indeed appeared consistent with gender discrimination. However, when researchers looked more closely within specific departments, this bias against women went away, and even reversed in several cases. More men were applying to the schools (as enginyering) soffering the   higher % of rejections. 

[Berkeley's data](/Users/albert/graphstables/SympsonParadoxBerkeleyTable.png )
 
[Simpson’s paradox in education, NAEP Math scores ](/Users/albert/graphstables/SympsonParadoxNAEPdecreaseOverall.png)


## Also in evaluating change

[When average isn’t good enough: Simpson’s paradox in education and earnings
Brad Her](https://www.brookings.edu/blog/social-mobility-memos/2015/07/29/when-average-isnt-good-enough-simpsons-paradox-in-education-and-earnings/)

....

[Change of earnings by years](/Users/albert/graphstables/SympsonParadoxEarnings.png)

It would be a shame if real progress in these areas was overlooked because of a naïve reliance on single averages

 
 
 
## Path models  (employee data)

```{r, echo=TRUE,fig.height = 3.5, fig.width = 6, fig.align = "center"}

  modelSE1<- "   jobsat  ~     age  + jobperf + female +  iq
 jobperf   ~  age+ jobsat + wbeing + 
       turnover + iq + turnover + female
"
(fitSE1 <- sem(modelSE1, data, missing = "fiml"))

```

## Path diagram (cont.)

```{r,echo=FALSE, fig.height = 3.5, fig.width = 6, fig.align = "center"}

semPaths(fitSE1, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, weighted=FALSE,intercepts = FALSE, 
    edge.label.position = 0.6, bg = "white", borders = TRUE,
 label.norm = "O", style = "lisrel", nCharNodes = 6,curve=2,curvature=1.4,combineGroups=FALSE,groups="manifest")
 

```



# Non-recursive latent variable model (Bagozzi's data, Perf and JobS)
 
![Performance and Satisfaction, Richard Bagozzi, Journal of Marketing](/Users/albert/courses/BSMsemcourse/slides/BagozziSalesSatis1.png )

## satisfaction and job performance (Baggozzi's data)

```{r, echo=TRUE,fig.height = 3.5, fig.width = 6, fig.align = "center"}

library(lavaan)
lower <- "
1.000
0.418 1
0.394 0.627 1.000
0.129 0.202 0.266 1.000
0.189 0.284 0.208 0.385 1.000
0.544 0.281 0.324 0.201 0.161 1.000
0.507 0.225 0.314 0.172 0.174 0.546 1.00
-0.357 -0.156 -0.038 -0.199 -0.277 -0.294 -0.174 1.00
"
cov <- getCov(lower, names = c("PERF", "JS1",
          "JS2", "AM1", "AM2","SE1","SE2", "VERB"))
SD <- c(209.06,3.34, 2.81,1.95,2.08,
    2.16, 2.06, 3.65)
  cova <- diag(SD)%*%cov%*%diag(SD)
  rownames(cova)<- colnames(cova)<- c("PERF", 
    "JS1", "JS2", "AM1", "AM2","SE1","SE2", "VERB")

```



## Moments unrestricted 

```{r, echo=FALSE,fig.height = 3.5, fig.width = 6, fig.align = "center"}
  
model <- "
 PERF ~~ PERF+ JS1+JS2+AM1+AM2+SE1+SE2+VERB 
 JS1 ~~ JS1  +JS2+AM1+AM2+SE1+SE2+VERB 
  JS2 ~~ JS2   + AM1+AM2+SE1+SE2+VERB 
    AM1 ~~ AM1    +AM2+SE1+SE2+VERB 
   AM2 ~~ AM2     +SE1+SE2+VERB 
     SE1 ~~ SE1    +SE2+VERB 
        SE2 ~~ SE2    +VERB 
            VERB  ~~ VERB   
  "
fit <- sem(model, sample.cov = cova, auto.var=TRUE, sample.nobs = 122)

library(semPlot)

semPaths(fit , "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, weighted=FALSE,
    intercepts = FALSE, edge.label.position = 0.7, bg = "white", borders = TRUE,
    label.norm = "O", style = "lisrel", nCharNodes = 6, layout= "circle", groups="manifest")

 

  
 


```



## Submodel: regression with error in variables

```{r,echo=FALSE, fig.height = 3.5, fig.width = 6, fig.align = "center"}
regLV<- "
# measurement of true satisfaction TS
TS =~ 1*JS1 + 1*JS2 # regression
PERF ~ TS "
fitregLV <- sem(regLV, sample.cov = cova, sample.nobs = 122) #parameterestimates(fitreg,standardized = TRUE, rsquare=TRUE, ci=FALSE)
 
```

## Submodel: path diagram

```{r, echo=FALSE,  fig.height = 3.5, fig.width = 6, fig.align = "center"}

library(semPlot)
semPaths(fitregLV, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, weighted=FALSE,intercepts = FALSE, 
    edge.label.position = 0.6, bg = "white", borders = TRUE,
 label.norm = "O", style = "lisrel", nCharNodes = 6,curve=2,curvature=1.4,combineGroups=FALSE,groups="latents")

```

## Full model (Bagozzi's data )

```{r}
model <- "
JS =~  JS1  + JS2
SE =~  SE1 + SE2
AM =~   AM1 + AM2
## regression 
JS ~  PERF  + AM
PERF ~ JS + SE + VERB
## covariances 
AM ~~ SE + VERB
SE ~~ SE + VERB
VERB ~~VERB
"
fit <- sem(model, sample.cov = cova, auto.var=TRUE, sample.nobs = 122)
 parameterEstimates(fit)[7:11, ]

```

## model fit 

```{r}

round(fitMeasures(fit)[c("chisq", "df",
                  "pvalue", "rmsea")],2)

```

## Path diagram (Bagozzi)

```{r, echo=FALSE, fig.height = 3.5, fig.width = 6, fig.align = "center"}


semPaths(fit, "std", sizeInt = 1, sizeMan = 8, edge.label.cex = 1, asize = 2, weighted=FALSE,intercepts = FALSE, 
    edge.label.position = 0.6, bg = "white", borders = TRUE,
 label.norm = "O", style = "lisrel", nCharNodes = 6,curve=2,curvature=1.4,combineGroups=FALSE,groups="latents")


```


## Non-recursive path analysis model with latent variables (Bagozzi's data)
\tiny
```{r, echo=FALSE,fig.height = 5, fig.width = 6, fig.align = "center"}
 
model <- "
JS =~  JS1  + JS2
SE =~  SE1 + SE2
AM =~   AM1 + AM2
## regression 
JS ~  PERF  + AM
PERF ~ JS + SE + VERB
## covariances 
AM ~~ SE + VERB
SE ~~ SE + VERB
VERB ~~VERB
"
fit <- sem(model, sample.cov = cova, auto.var=TRUE, sample.nobs = 122)
#summary(fit)
 parameterEstimates(fit) 
 
```

 


# References

1.  
[Basic lavaan Syntax Guide, by James B. Grace](http://www.structuralequations.com/resources/Basic_lavaan_Syntax_Guide_Aug1_2013.pdf)



2. [Yves Rosseel, 2019, Tutorial Lavaan](http://lavaan.ugent.be/tutorial/tutorial.pdf)

3. [Basic lavaan Syntax Guide: Estimators](http://lavaan.ugent.be/tutorial/est.html)


4.  Fuller, W. (1987).  _Measurement Error Models_, Wiley

5.   [Sociometric Research Foundation](https://www.sociometricresearchfoundation.com)

6.  [Structural Equation Modeling in Open-Source Software ](https://scholarworks.iu.edu/dspace/bitstream/handle/2022/21825/2017-12-01_wim_wild_sem_slides.pdf?sequence=2&isAllowed=y)

   
    
 
